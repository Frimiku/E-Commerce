# 电子商务项目（Spark+Hive）

[TOC]

## 一、项目介绍

### 1.1、数仓介绍

```shell
# spark-sql 异常提示: 无struct处出现struct异常，通常【字段列表中缺少英文逗号】
数仓：全称为“数据”“仓库”，是一种用数据治理企业的方案。
-- 数据：描述客户事物的符号
	数据描述的主体：
		静态维度数据：RDB
		动态事实数据：文件/API，行为数据，过程数据
	数据结构: API
		结构化：
			JDBC: 关系型数据库：RDB(create table(id:int,name:varchar,...)) 
		半结构化
			解析数据方法：FastJson/Jackson => JSON：{"id":1,"name":"张三丰",...}
			解析数据方法：DOM4J => XML: 标签
                            <employees>
                                <employee id="1">
                                    <name>张三丰</name>
                                    ...
                                </employee>
                                <employee id="2">
                                    <name>赵敏</name>
                                    ...
                                </employee>
                            </employees>
			解析数据方法：正则分组提取
				2024-05-15 10:07:38 INFO EmpController.login() ...
				2024-05-15 10:07:38|张三,26/....
		非结构化
			图片，音频，视频，普通文本
-- 仓库：
	主要功能：存储（如：粮仓存粮食，数仓存数据）
	数据仓库：
		数据很多，就如粮食很多，一个库存存不下，该如何解决？
		解决方案：多建库，建几个合适？
		解决方案：统计往年的粮食总量，以及历年的粮食变化趋势，结合当前的种粮现状
		存储方式：
			集中存储：
				只能使用某一台或某几台算力，算力就会成为瓶颈
				若要使用集群的总算力，数据迁移，网络就会成为瓶颈
			分布式存储：计算时数据尽量数据本地化 ✔
				同一份数据，切成数据块(block:TB/128M)
		分布式计算 ✔
			充分利用集群内所有机器的总算力
		大数据的计算：
			数据问题：结构差异化，数据质量，...
			算力再强，基于原始大数据计算，效率一定很低。
			分层：
				1、提前将需要提取的数据准备好
				2、大数据：结构化差异，数据表（多），每张表的字段多，每张表数据行数多，且数据存在荣誉
				3、数据来源和提取数据权限
		✔ 规范化
			数据管控权
			表多，字段多，数据多：管理采用分治法
				大数据数仓建设采用分层思想
				1、分库：一层一库
				2、分工：一层一工
		


✔ 售企业最近1年门店收集的数据进行数据分析
    1、潜在客户画像
    2、用户消费统计
    3、门店的资源利用率
    4、消费的特征人群定位
    5、数据的可视化展现
```

### 1.2、数仓架构（基本流程）

```
数仓架构（基本流程）：
	1、数据采集(Extract)：		
		数据流采集工具：Flume ✔, RDB: Sqoop,Maxwell,Canal,...
	2、数据存储(Load)：
		一：【近源层ODS】：Transform					Operation Data Store
			csv, rdb-table => 映射（表对表，字段对字段，数据类型转换）
			json => 单列数据表
		二：公共层CDM：								Common Data Model
			维度数据：DIM
			【明细层DWD】：Aggregation				行数不变
				模型：降维（最多星型），鼓励数据冗余
				列裁剪，清洗(残缺，错误，格式)
				若数据存在问题，需要排查，比重
					若比重过大，为数据源问题
					若比较较小，且为正常问题
						残缺：填充之后要保证正态分布
							性别残缺，根据比重，若女比较为0.4
								if(femaleRatio<=0.4,'女','男')
							数值残缺，均值，中位数，百分位数
			【轻聚层DWS】：index data/index source
				预聚合，重复使用
					聚合方式，是根据主题域，主题，指标的全方位分析，
					从而总结归纳出
					  共性维度，
						单一维度：仅日期
						组合维度：品类与日期
					  梯度
						时间：时，时段，天，周，月，季，年
						品类：三级分类
				
				多重分组并表
					select a,b,c,sum(pay) as sum_pay
					
					group by a,b,c grouping sets(a,(a,c),(b,c))
					----------------------------------------------
					grouping__id, a, 	b, 		c, 		sum_pay
					-----------------------------------------------
					1,            a1,	null, 	null, 	2598890.89
					1,			  a2,   null, 	null,	827738.76
					2,			  a3,	null,	c1,		7726638.00
					3,			  null,	b1,		c2,		662773.66
					
				select a,b,c,sum(pay) as sum_pay
				rollup(a,b,c) <=> group by a,b,c grouping sets(a,(a,b),(a,b,c))
				
				select a,b,c,sum(pay) as sum_pay
				cube(a,b,c) <=> 
					group by a,b,c grouping sets(a,b,c,(a,b),(a,c),(b,c),(a,b,c))
				
				做数据字典：
					grouping__id	group by
					1				year, month
					2				sku, year
				
				放在一张表的多重分组，字段数量差别限制
					DWD -> 8中分组方式 -> 2,4,5 / 6,6,7,8
					
		三：【应用层ADS】：								Application Data Service
			一类数据一张表，无需再聚合
	
	3、结果数据导出：mysql ✔,canal,sqoop
	
	4、数据呈现（BI）：BI(Grafana,FineReport ✔,...)
   
架构应用技术：
   	Flume : 主要采集CSV, JSON数据
   	Mysql : 数仓元数据
   	Hadoop（HDFS）: 实体数据
   	Hive  : 数仓
   		需知：Hive 默认采用的是：HDFS分布式存储，MapReduce计算引擎
   		应用方式：
            方式一：Hive on Spark ：Hive官方版本中默认不支持 Spark 引擎
                Githup => 下载对应版本 hive 源码(maven) => 添加spark依赖 => 重新编译打包
            方式二：Spark on Hive : Hive 仅作为 Spark 的数据源（数据存储：HDFS+MySQL） ✔
                spark sql 算子(RDD, SQL)
                spark sql
   	Spark：计算引擎
   	Zeppelin 0.10 | IDEA ：做数仓的平台
   	FineReport：数据呈现（BI）
```

### 1.3、数据结构

#### 1.3.1、客户数据表 Customer

| **customer_details** | **details**                       | remark     |
| -------------------- | --------------------------------- | ---------- |
| customer_id          | Int, 1 - 500                      | 客户编号   |
| first_name           | string                            | 名         |
| last_name            | string                            | 姓         |
| email                | string, such as willddy@gmail.com | 邮箱       |
| gender               | string, Male or female            | 性别       |
| address              | string                            | 地址       |
| country              | string                            | 国家       |
| language             | string                            | 语种       |
| job                  | string, job title/position        | 工种       |
| credit_type          | string                            | 信用卡类型 |
| credit_no            | string                            | 信用卡号   |

#### 1.3.2、交易表 Transaction

| **transaction_details** | **details**                   | remark   |
| ----------------------- | ----------------------------- | -------- |
| transaction_id          | Int, 1 - 1000                 | 交易编号 |
| customer_id             | Int, 1 - 500                  | 客户编号 |
| store_id                | Int, 1 - 5                    | 店铺编号 |
| price                   | decimal, such as 5.08         | 单价     |
| product                 | string, things bought         | 产品     |
| date                    | string, when to purchase      | 交易日期 |
| time                    | string, what time to purchase | 交易时间 |

#### 1.3.3、店铺表 Store

| **store_details** | **details** | remark   |
| ----------------- | ----------- | -------- |
| store_id          | Int, 1 - 5  | 店铺编号 |
| store_name        | string      | 店铺名称 |
| employee_number   | Int         | 雇员数量 |

#### 1.3.4、评价表 Review

| **store_review** | **details**   | remark   |
| ---------------- | ------------- | -------- |
| transaction_id   | Int, 1 - 8000 | 交易编号 |
| store_id         | Int, 1 - 5    | 店铺编号 |
| review_score     | Int, 1 - 5    | 评分     |



## 二、准备工作

### 2.1、数据上传

在single虚拟机中进行此处操作

需要上传的数据文件如下：

> `客户相关的数据文件（customer_details.csv）`，
>
> `店铺相关的数据文件（store_details.csv）`，
>
> `交易相关的数据文件（transaction_details.csv）`，
>
> `评价相关的数据文件（store_review.csv）`。

```shell
# 数仓父目录
mkdir project_ec
cd project_ec

# 数据源
mkdir datasource

# 将数据上传至虚拟机
# 分别存储：客户，店铺，评价，交易数据，至对应的目录下
cd datasource
mkdir customer store review transaction
```

### 2.2、数据检查

在single虚拟机中进行此处操作

#### 2.2.1、客户 - 数据检查

```shell
# 进入datasource目录下：cd /root/project_ec/datasource/
wc customer/customer_details.csv
-------------------------------------------------
501  2326 63686 customer/customer_details.csv
-------------------------------------------------

head -5 customer/customer_details.csv
-------------------------------------------------------------------------------------
customer_id,first_name,last_name,email,gender,address,country,language,job,credit_type,credit_no
1,Spencer,Raffeorty,sraffeorty0@dropbox.com,Male,9274 Lyons Court,China,Khmer,Safety Technician III,jcb,3.58937E+15
2,Cherye,Poynor,cpoynor1@51.la,Female,1377 Anzinger Avenue,China,Czech,Research Nurse,instapayment,6.37659E+15
...
-------------------------------------------------------------------------------------
# 501 行，首行为表头，500行数据
```

#### 2.2.2、交易 - 数据检查

```shell
# 进入datasource目录下：cd /root/project_ec/datasource/
wc transaction/transaction_details.csv
--------------------------------------------------------
8101  38140 461795 transaction/transaction_details.csv
--------------------------------------------------------

head -3 transaction/transaction_details.csv
-------------------------------------------------------------------------------------
transaction_id,customer_id,store_id,price,product,date,time
1,225,5,47.02,Bamboo Shoots - Sliced,2018/8/4,8:18
2,290,5,43.12,Tarts Assorted,2018/9/23,14:41
-------------------------------------------------------------------------------------

# 8101 行，首行表头，8100个交易
# 注意到：交易表中 transaction_id Int, 1 - 8000，数据可能有问题，进 dwd 之前解决
```

#### 2.2.3、店铺 - 数据检查

```shell
# 进入datasource目录下：cd /root/project_ec/datasource/
wc store/store_details.csv
-----------------------------------
6   6 111 store/store_details.csv
-----------------------------------

head -5 store/store_details.csv
-------------------------------------------------------------------------------------
store_id,store_name,employee_number
1,NoFrill,10
2,Lablaws,23
3,FoodMart,18
4,FoodLovers,26
-------------------------------------------------------------------------------------

# 6行，首行表头，5个店铺
```

#### 2.2.4、评价 - 数据检查

```shell
# 进入datasource目录下：cd /root/project_ec/datasource/
wc review/store_review.csv
----------------------------------------
1001 1001 9833 review/store_review.csv
----------------------------------------

head -3 review/store_review.csv
-------------------------------------------------------------------------------------
transaction_id,store_id,review_score
7430,1,5
912,3,3
-------------------------------------------------------------------------------------

# 1001行，首行表头，1000条评价
```

### 2.3、项目依赖配置

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.ybg</groupId>
    <artifactId>warehouse_ebs</artifactId>
    <version>1.6</version>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
        <!-- 所需依赖的版本 -->
        <spark.version>3.1.2</spark.version>
        <spark.scala.version>2.12</spark.scala.version>
        <hadoop.version>3.1.3</hadoop.version>
        <mysql.version>8.0.33</mysql.version>
        <hive.version>3.1.2</hive.version>
        <hbase.version>2.3.5</hbase.version>
        <jackson.version>2.10.0</jackson.version>
    </properties>

    <dependencies>
        <!-- spark-core -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <!-- spark-sql -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <!-- spark-hive -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${spark.scala.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <!-- hadoop-common -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
        </dependency>

        <!-- mysql -->
        <dependency>
            <groupId>com.mysql</groupId>
            <artifactId>mysql-connector-j</artifactId>
            <version>${mysql.version}</version>
        </dependency>

        <!-- hive-exec -->
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-exec</artifactId>
            <version>${hive.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.logging.log4j</groupId>
                    <artifactId>log4j-slf4j-impl</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <!-- HBase 驱动 -->
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>${hbase.version}</version>
        </dependency>

        <!-- jackson-databind -->
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-core</artifactId>
            <version>${jackson.version}</version>
        </dependency>

        <!-- jackson-databind -->
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
            <version>${jackson.version}</version>
        </dependency>
    </dependencies>

    <!--  pom包  -->
    <build>
        <plugins>
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                    <archive>
                        <!-- 可执行 jar -->
                        <!--<manifest>
                                <mainClass>com.ybg.dfs.StoryWordCount2</mainClass>
                        </manifest>-->
                        <addMavenDescriptor>false</addMavenDescriptor>
                        <manifest>
                            <addClasspath>true</addClasspath>
                        </manifest>
                    </archive>

                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>4.3.1</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```

### 2.4、工程核心组件搭建

#### 2.3.1、共性模块

```scala
package core

import java.time.LocalDateTime
import java.time.format.DateTimeFormatter
import java.time.temporal.ChronoField
import scala.collection.mutable.ArrayBuffer

object Validator {

  /**
   * 数据校验【安全性判断】
   * @param title 校验主题[配置项名称]
   * @param value 待校验值
   * @param regex 若待校验值为字符串，且有特定的规则，提供正则表达式进一步验证格式
   */
  def check(title:String,value:Any,regex:String=null)={
    // 空指针判断
    if (null==value){
      throw new RuntimeException(s"value for $title null pointer exception")
    }
    // value为字符串时，判断是否为空
    if (value.isInstanceOf[String]){
      if (value.toString.isEmpty){
        throw new RuntimeException(s"value for $title empty string exception")
      }
      // 正则匹配与否判断
      if (null != regex && !value.toString.matches(regex)){
        throw new RuntimeException(s"value for $title not match regex $regex exception")
      }
    }
  }

  /**
   * 日期维度数据生成：从开始时间到结束时间的期间时间生成
   */
  case class DimDate(tran_year:Int,tran_quarter: Int,tran_month:Int,tran_month_week:Int,tran_day:Int,tran_range:Int)
  // 参数：开始日期，结束时间
  def dimDate(dateBegin:String,dateEnd:String):Seq[DimDate] = {
    var begin = LocalDateTime.parse(s"${dateBegin}T00:00:00")
    val end = LocalDateTime.parse(s"${dateEnd}T00:00:00")
    val buffer = ArrayBuffer[DimDate]()
    // isBefore => 小于
    while(begin.isBefore(end)){
      val year: Int = begin.getYear
      val month: Int = begin.getMonthValue
      val quarter: Int = (month - 1) / 3 + 1
      val monthWeek: Int = begin.get(ChronoField.ALIGNED_WEEK_OF_MONTH)
      val day: Int = begin.getDayOfMonth
      val range: Int = begin.getHour/3+1
      buffer.append(DimDate(year,quarter,month,monthWeek,day,range))
      begin = begin.plusHours(3)
    }
    buffer
  }

  // UDF: 修补日期，转为标准格式
  def dateFormat(datetime:String) :String= {
    val arr: Array[String] = datetime.split(" ")
    //年月日
    val ymd: String = arr(0)
      .split("-")
      .map(_.toInt)
      .map(d => s"${if (d < 10) "0" else ""}$d")
      .mkString("-")

    // 时分秒
    var bool = false
    val hms: String = arr(1)
      .split(":")
      .map(_.toInt)
      .zipWithIndex
      .map(d =>
        if (d._2 == 0 && d._1 >= 24) {
          bool = true
          d._1 - 24
        }
        else
          d._1
      )
      .map(d => s"${if (d < 10) "0" else ""}$d")
      .mkString(":")
    // 拼接为一个标准日期
    val date: String = Array(ymd, hms).mkString("T")

    LocalDateTime
      .parse(date)
      .plusDays(if(bool) 1 else 0)
      .format(DateTimeFormatter.ISO_LOCAL_DATE_TIME)
  }

}
```

#### 2.3.2、Spark核心组件

```scala
package core

import core.SparkFactory.Builder
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

import core.Validator.check

// 一般而言：将数据导入【入口】，将hive中数据导入进行操作[多进一出] | 出口
// hive与spark连接[spark和hive启起来]
class SparkFactory {
  def build():Builder={
    new Builder {
      val conf:SparkConf = new SparkConf()

      /**
       * 单项配置
       * @param item 配置项名称
       * @param value 配置项值
       * @param regexValue 配置项值正则规则
       */
      private def set(item:String,value:String,regexValue:String=null)={
        check("name_of_config_item",item,"^spark\\..*")
        check(item,value,regexValue)
        conf.set(item,value)
      }

      // 基本配置
      private def setBaseAppName(appName:String)={
        set("spark.app.name",appName,"^\\w+$")
      }
      private def setBaseMaster(master:String)={
        set("spark.master",master,"local(\\[(\\*|[1-9][0-9]*)])?|spark://([a-z]\\w+|\\d{1,3}(\\.\\d{1,3}){3}):\\d{4,5}|yarn")
      }
      private def setBaseDeployMode(deployMode:String) ={
        set("spark.submit.deployMode",deployMode,"client|cluster")
      }
      private def setBaseEventLogEnabled(eventLogEnabled:Boolean) ={
        set("spark.eventLog.enabled",s"$eventLogEnabled")
      }
      def baseConfig(appName:String,master:String="local[*]",deployMode:String="client",eventLogEnabled:Boolean=false):Builder={
        setBaseAppName(appName)
        setBaseMaster(master)
        /*
         *  在集群中（standalone|yarn）某一台机器上发布驱动端：
         *  client：本地模式
         *  cluster：远程模式
         */
        setBaseDeployMode(deployMode)
        /*
         * 构建 Web UI
         */
        setBaseEventLogEnabled(eventLogEnabled)
        this
      }

      // 驱动端优化
      private def setDriverMemory(memoryGB:Int)={
        set("spark.driver.memory",s"${memoryGB}g","[1-9]\\d*g")
      }
      private def setDriverCoreNum(coreNum:Int)={
        set("spark.driver.cores",s"$coreNum","[1-9]\\d*")
      }
      private def setDriverMaxResultGB(maxRstGB:Int)={
        set("spark.driver.maxResultSize",s"${maxRstGB}g","[1-9]\\d*g")
      }
      private def setDriverHost(driverHost:String)={
        set("spark.driver.host",driverHost,"localhost|[a-z]\\w+")
      }
      def optimizeDriver(memoryGB:Int=2,coreNum:Int=1,maxRstGB:Int=1,driverHost:String="localhost"):Builder={
        setDriverMemory(memoryGB)
        setDriverCoreNum(coreNum)
        // 每一个spark行动算子触发的所有分区序列化结果大小上线
        setDriverMaxResultGB(maxRstGB)
        // standalone 模式需要配置 driverHost，便于 executor 与 master 通信
        if (conf.get("spark.master").startsWith("spark://")){
          setDriverHost(driverHost)
        }
        this
      }

      // 执行端优化
      private def setExecutorMemory(memoryGB:Int)={
        set("spark.executor.memory",s"${memoryGB}g","[1-9]\\d*g")
      }
      private def setExecutorCoreNum(coreNum:Int)={
        set("spark.executor.cores",s"$coreNum","[1-9]\\d*")
      }
      def optimizeExecutor(memoryGB:Int=1,coreNum:Int=1):Builder={
        setExecutorMemory(memoryGB)
        /*
         * yarn模式下只能 1 个核
         * 其他模式下，所有可用的核
         */
        if (conf.get("spark.master").startsWith("spark://")){
          setExecutorCoreNum(coreNum)
        }
        this
      }

      // 限制优化
      private def setLimitMaxCores(maxCores:Int)={
        set("spark.cores.max",s"$maxCores","[1-9]\\d*")
      }
      private def setLimitMaxTaskFailure(maxTaskFailure:Int)={
        set("spark.task.maxFailures",s"$maxTaskFailure","[1-9]\\d*")
      }
      private def setLimitMaxLocalWaitS(maxLocalWaitS:Int)={
        set("spark.locality.wait",s"${maxLocalWaitS}s","[1-9]\\d*s")
      }
      def optimizeLimit(maxCores:Int=4,maxTaskFailure:Int=3,maxLocalWaitS:Int=3):Builder={
        if (conf.get("spark.master").startsWith("spark://")){
          setLimitMaxCores(maxCores)
        }
        /**
         * 单个任务允许失败的最大次数，超出会杀死本次 job ，重试，最大重试次数为：此值-1
         */
        setLimitMaxTaskFailure(maxTaskFailure)
        /**
         * 数据本地化读取加载的最大等待时间
         * 大任务：建议适当增加此值
         */
        setLimitMaxLocalWaitS(maxLocalWaitS)
        this
      }

      // 序列化优化
      def optimizeSerializer(serde:String="org.apache.spark.serializer.JavaSerializer", clas:Array[Class[_]]=null):Builder={

        /**
         * 设置将需要通过网络发送或快速缓存的对象序列化的工具类
         * 默认为 Java 序列化：org.apache.spark.serializer.JavaSerializer
         * 为了提速，推荐设置为：org.apache.spark.serializer.KryoSerializer
         * 若采用 KryoSerializer 序列化方式，需要将所有自定义的实体类(样例类)注册到配置中心
         */
        set("spark.serializer",serde,"([a-z]+\\.)+[A-Z]\\w*")
        if (serde.equals("org.apache.spark.serializer.KryoSerializer")){
          conf.registerKryoClasses(clas)
        }
        this
      }

      // 网络相关优化
      private def setNetTimeout(netTimeoutS:Int)={
        set("spark.network.timeout",s"${netTimeoutS}s","[1-9]\\d*s")
      }
      private def setNetSchedulerMode(schedulerMode:String) ={
        set("spark.scheduler.mode",schedulerMode,"FIFO|FAIR")
      }
      def optimizeNetAbout(netTimeoutS:Int=120,schedulerMode:String="FIFO"):Builder={
        /**
         * 所有和网络交互相关的超时阈值
         */
        setNetTimeout(netTimeoutS)
        /**
         * 多人工作模式下：建议设置为 FAIR
         */
        setNetSchedulerMode(schedulerMode)
        this
      }

      // 动态分配优化
      private def setDynamicEnabled(dynamicEnabled:Boolean) ={
        set("spark.dynamicAllocation.enabled",s"$dynamicEnabled")
      }
      private def setDynamicInitialExecutors(initialExecutors:Int)={
        set("spark.dynamicAllocation.initialExecutors",s"$initialExecutors","[1-9]\\d*")
      }
      private def setDynamicMaxExecutors(maxExecutors:Int)={
        set("spark.dynamicAllocation.maxExecutors",s"$maxExecutors","[1-9]\\d*")
      }
      private def setDynamicMinExecutors(minExecutors:Int)={
        set("spark.dynamicAllocation.minExecutors",s"$minExecutors","[0-9]\\d*")
      }
      def optimizeDynamicAllocation(dynamicEnabled:Boolean=false,initialExecutors:Int=3,minExecutors:Int=0,maxExecutors:Int=6):Builder={
        // 只有 dynamicEnabled为true才可继续进行
        /**
         * 根据应用的工作需求，动态分配 Executors 资源
         */
        if (dynamicEnabled){
          setDynamicEnabled(true)
          setDynamicInitialExecutors(initialExecutors)
          setDynamicMinExecutors(minExecutors)
          setDynamicMaxExecutors(maxExecutors)
        }
        this
      }

      // shuffle优化
      private def setShuffleParallelism(parallelism:Int)={
        set("spark.default.parallelism",s"$parallelism","[1-9]\\d*")
      }
      private def setShuffleCompressEnabled(shuffleCompressEnabled:Boolean)={
        set("spark.shuffle.compress",s"$shuffleCompressEnabled")
      }
      private def setShuffleMaxSizePerReducer(maxSizeMB:Int)={
        set("spark.reducer.maxSizeInFlight",s"${maxSizeMB}m","[1-9]\\d*m")
      }
      private def setShuffleServiceEnabled(shuffleServiceEnabled:Boolean)={
        set("spark.shuffle.service.enabled",s"$shuffleServiceEnabled")
      }
      def optimizeShuffle(parallelism:Int=3,shuffleCompressEnabled:Boolean=false,maxSizeMB:Int=128,shuffleServiceEnabled:Boolean=true):Builder={
        /**
         * 如果用户没有指定分区数（numPar|Partitioner），则采用该值作为默认的分区数
         */
        setShuffleParallelism(parallelism)
        /**
         * Shuffle 过程中 Map 端输出数据是否压缩，建议生成过程中，数据规模较大时启动
         */
        setShuffleCompressEnabled(shuffleServiceEnabled)
        /**
         * 设置Reducer端缓冲区大小,生成环境中，服务器内存较大时，可以适当增大【本地计算时需关闭，为false】
         */
        setShuffleMaxSizePerReducer(maxSizeMB)
        /**
         * 开启一个独立外部服务，专门存储 Executor 产生的数据
         */
        setShuffleServiceEnabled(shuffleServiceEnabled)
        this
      }

      // 推测执行
      private def setSpeculationEnabled(speculationEnabled:Boolean)={
        set("spark.speculation",s"$speculationEnabled")
      }
      private def setSpeculationIntervalS(intervalS:Int)={
        set("spark.speculation.interval",s"${intervalS}s","[1-9]\\d*s")
      }
      private def setSpeculationQuantile(quantile:Float)={
        set("spark.speculation.quantile",s"${quantile}","0?\\.\\d+")
      }
      def optimizeSpeculation(speculationEnabled:Boolean=false,intervalS:Int=5,quantile:Float=0.75f):Builder={
        if (speculationEnabled){
          /**
           * 是否开启推测执行服务,将各阶段中(Stage)中执行慢的任务(Task)重启
           */
          setSpeculationEnabled(true)
          /**
           * 推测频次 intervalS(秒/次)
           */
          setSpeculationIntervalS(intervalS)
          /**
           * 开启推测执行前，任务的完成比列
           */
          setSpeculationQuantile(quantile)
        }
        this
      }

      // 运行时优化
      // 数据倾斜优化
      private def setAdaptiveEnabled(adaptiveEnabled:Boolean)={
        set("spark.sql.adaptive.enabled",s"$adaptiveEnabled")
      }
      // 设置自动广播大小
      private def setAutoBroadCastThreshold(thresholdMB:Int)={
        set("spark.sql.autoBroadcastJoinThreshold",s"${thresholdMB}MB","[1-9]\\d*MB")
      }
      // 根据消耗情况来判断是否进行优化（默认关闭） => 一般打开【了解】
      private def setCBOEnabled(enabled:Boolean)={
        set("spark.sql.cbo.enabled",s"$enabled")
      }
      def optimizeRuntime(adaptiveEnabled:Boolean=false, threshold:Int=10, cboEnabled:Boolean)={
        /**
         * spark.sql.adaptive.skewJoin.enabled 默认为 true
         * 但只有当 spark.sql.adaptive.enabled(默认为 false) 设置为 true 时生效
         */
        setAdaptiveEnabled(adaptiveEnabled)
        setAutoBroadCastThreshold(threshold)
        setCBOEnabled(cboEnabled)
        this
      }

      // SparkStreaming背压机制
      // 是否开启背压机制(默认false)
      private def setBackpressureEnabled(backpressureEnabled:Boolean)={
        set("spark.streaming.backpressure.enabled",s"$backpressureEnabled")
      }
      // 全局设置，限制所有接收器的最大接受频率
      private def setBackpressureReceiverMaxRate(receiverMaxRate:Int)={
        set("spark.streaming.receiver.maxRate",s"$receiverMaxRate")
      }
      // 针对kafka特定设置，限制每个kafka分区的最大接受速率
      private def setBackpressureMaxRatePerPartition(maxRatePerPartition:Int)={
        set("spark.streaming.kafka.maxRatePerPartition",s"$maxRatePerPartition")
      }
      def backpressure(backpressureEnabled:Boolean=false, receiverMaxRate:Int=5000, maxRatePerPartition:Int=1500)={
        setBackpressureEnabled(backpressureEnabled)
        setBackpressureReceiverMaxRate(receiverMaxRate)
        setBackpressureMaxRatePerPartition(maxRatePerPartition)
        this
      }

      // 数仓路径(根目录在哪里)
      def warehouseDir(hdfs:String):Builder={
        set("spark.sql.warehouse.dir",hdfs,"hdfs://([a-z]\\w+|\\d{1,3}(\\.\\d{1,3}){3}):\\d{4,5}(/\\w+)+")
        this
      }

      // 生成SparkSession
      def end():SparkSession={
        SparkSession
          .builder()
          .config(conf)
          .enableHiveSupport() // 用途：sparkSql关联hive【需要四个配置】
          .getOrCreate()
      }

    }
  }
}
object SparkFactory{
  def apply(): SparkFactory = new SparkFactory()

  trait Builder{
    // 基本配置
    /**
     * 基本配置
     * @param appName 名称
     * @param master
     * @param deployMode  发布者模式
     * @param eventLogEnabled 事件日志模式【发布时打开,测试时无需打开】
     * @return
     */
    def baseConfig(appName:String,master:String="local[*]",deployMode:String="client",eventLogEnabled:Boolean=false):Builder
    // 以下为优化配置
    /**
     * 驱动端优化
     * @param memoryGB 内存gb
     * @param coreNum 核数
     * @param maxRstGB 最大结果大小
     * @param driverHost diver端配置
     * @return
     */
    def optimizeDriver(memoryGB:Int=2,coreNum:Int=1,maxRstGB:Int=1,driverHost:String="localhost"):Builder
    /**
     * 执行端优化
     * @param memoryGB 内存gb
     * @param coreNum 核数
     * @return
     */
    def optimizeExecutor(memoryGB:Int=1,coreNum:Int=1):Builder
    /**
     * 限制优化
     * @param maxCores 最大核数
     * @param maxTaskFailure 单个task任务失败的最大次数(容错)。超出会杀死本次 job ，重试，最大重试次数为：此值-1
     * @param maxLocalWaitS 最多等待时间（秒）
     * @return
     */
    def optimizeLimit(maxCores:Int=4,maxTaskFailure:Int=3,maxLocalWaitS:Int=3):Builder
    /**
     * 序列化优化
     * @param serde 全包路径（class文件的全包路径）
     * @param clas 多个class
     * @return
     */
    def optimizeSerializer(serde:String="org.apache.spark.serializer.JavaSerializer", clas:Array[Class[_]]=null):Builder
    /**
     * 网络相关优化
     * @param netTimeoutS 网络超时(秒)
     * @param schedulerMode 消息队列[如何分配给任务资源],此处是平等分配资源
     * @return
     */
    def optimizeNetAbout(netTimeoutS:Int=120,schedulerMode:String="FIFO"):Builder
    /**
     * 资源动态分配优化[按需(要)分配]
     * @param dynamicEnabled 是否开启动态分配
     * @param initialExecutors 初始化容器数
     * @param minExecutors 最小保留容器数
     * @param maxExecutors 最多容器数
     * @return
     */
    def optimizeDynamicAllocation(dynamicEnabled:Boolean=false,initialExecutors:Int=3,minExecutors:Int=0,maxExecutors:Int=6):Builder
    /**
     * shuffle优化
     * @param parallelism 并行度【分区数】
     * @param shuffleCompressEnabled 资源是否需要压缩
     * @param maxSizeMB 单词传输的字节上限[一个block大小]
     * @param shuffleServiceEnabled 是否开启shuffle，yarn跑的时候需要开启true【本地计算时需关闭，为false】
     * @return
     */
    def optimizeShuffle(parallelism:Int=3,shuffleCompressEnabled:Boolean=false,maxSizeMB:Int=128,shuffleServiceEnabled:Boolean=true):Builder
    /**
     * 推测执行（若检查其性能不行，干掉重开）
     * @param speculationEnabled 是否开启测试推测模式
     * @param intervalS 多长时间检查一次（秒），即：推测频次【定期得出一个均值】
     * @param quantile 开启推测执行前，任务的完成比列
     * @return
     */
    def optimizeSpeculation(speculationEnabled:Boolean=false,intervalS:Int=5,quantile:Float=0.75f):Builder
    /**
     * 运行时优化
     * @param adaptiveEnabled 数据倾斜优化
     * @param threshold 设置自动广播大小
     * @param cboEnabled 根据消耗情况来判断是否进行优化（默认关闭）=> 一般打开
     * @return
     */
    def optimizeRuntime(adaptiveEnabled:Boolean=false, threshold:Int=10, cboEnabled:Boolean):Builder
    /**
     * SparkStreaming背压机制
     * @param backpressureEnabled 是否开启背压机制(默认false)
     * @param receiverMaxRate 全局设置，限制所有接收器的最大接受频率
     * @param maxRatePerPartition 针对kafka特定设置，限制每个kafka分区的最大接受速率
     * @return
     */
    def backpressure(backpressureEnabled:Boolean=false, receiverMaxRate:Int=5000, maxRatePerPartition:Int=1500):Builder
    // 数仓路径
    def warehouseDir(hdfs:String):Builder
    // 生成SparkSession
    def end():SparkSession
  }
}
```

#### 2.3.3、Mysql核心配置

```scala
package core


import core.MysqlConfigFactory.{Getter, Setter}
import core.Validator.check

import java.util.Properties


// 一般而言：数据的出口【将结果数据导出至mysql中】[多进多出] | 出口
class MysqlConfigFactory{
  def build():Setter = {
    new Setter {

      val conf = new Properties()

      override def setDriver(driverCla: String): Setter = {
        check("name_of_mysql_driver_class",driverCla,"com\\.mysql(\\.cj)?\\.jdbc\\.Driver")
        conf.setProperty("driver",driverCla)
        this
      }

      override def setUrl(url: String): Setter = {
        check("url_to_connect_mysql",url,"jdbc:mysql://([a-z]\\w+|\\d{1,3}(\\.\\d{1,3}){3}):\\d{4,5}/[a-z]\\w+(\\?.+)?")
        conf.setProperty("url",url)
        this
      }

      override def setUser(user: String): Setter = {
        check("user_to_connect_mysql",user)
        conf.setProperty("user",user)
        this
      }

      override def setPassword(password: String): Setter = {
        check("password_to_connect_mysql",password)
        conf.setProperty("password",password)
        this
      }

      override def finish(): Getter = {
        new Getter {
          override def getUrl: String = conf.getProperty("url")

          override def getConf: Properties = conf
        }
      }
    }
  }
}
object MysqlConfigFactory{
  def apply(): MysqlConfigFactory = new MysqlConfigFactory()

  trait Getter{
    def getUrl:String
    def getConf:Properties
  }

  trait Setter{
    // 字节码文件全包路径
    def setDriver(driverCla:String):Setter
    def setUrl(url:String):Setter
    def setUser(user:String):Setter
    def setPassword(password:String):Setter
    def finish():Getter
  }
}
```



## 三、环境安装 - 服务

```shell
# 安装
cd /opt/software
ll
---------------------------------------------------------------
total 11
drwxr-xr-x  13 dolphinscheduler dolphinscheduler  188 Jul 30 13:47 dolphinscheduler-2.0.8
drwxr-xr-x   8 root  root   198 Jan 16 22:45 flume-1.9.0
drwxr-xr-x  11 henry henry  173 Oct 15  2023 hadoop-3.1.3
drwxr-xr-x   9 root  root   205 Mar  2 16:51 hbase-2.3.5
drwxr-xr-x  11 root  root   196 Nov  8  2023 hive-3.1.2
drwxr-xr-x.  8    10   143  255 Mar 29  2018 jdk1.8.0_171
drwxr-xr-x   8 root  root   117 Mar 29 14:44 kafka_2.12-2.8.0
drwxr-xr-x  7 root   root   97  Nov 12  2023 mysql8
drwxr-xr-x  15 henry henry  235 Mar 15 09:12 spark-3.1.2
drwxr-xr-x  14   501 wheel  289 Apr 11 09:11 zeppelin-0.10.0
drwxr-xr-x   8 root  root   160 Jan 14 21:38 zookeeper-3.6.3
---------------------------------------------------------------

# 环境变量
vim /etc/profile.d/myenv.sh
--------------------------------------------------------------------------
# jdk 1.8
JAVA_HOME=/opt/software/jdk1.8.0_171
PATH=$PATH:$JAVA_HOME/bin
# hadoop 3.1.3
export HADOOP_HOME=/opt/software/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/lib
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export HDFS_JOURNALNODE_USER=root
export HDFS_ZKFC_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_LIBEXEC_DIR=$HADOOP_HOME/libexec
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
#hive 3.1.2
export HIVE_HOME=/opt/software/hive-3.1.2
export PATH=$PATH:$HIVE_HOME/bin
#zeppelin 0.10.0
export ZEPPELIN_HOME=/opt/software/zeppelin-0.10.0
export PATH=$PATH:$ZEPPELIN_HOME/bin
#zookeeper 3.6.3
export ZOOKEEPER_HOME=/opt/software/zookeeper-3.6.3
export PATH=$PATH:$ZOOKEEPER_HOME/bin
#hbase 2.3.5
export HBASE_HOME=/opt/software/hbase-2.3.5
export PATH=$PATH:$HBASE_HOME/bin
#spark-3.1.2
export SPARK_HOME=/opt/software/spark-3.1.2
export PATH=$SPARK_HOME/bin:$PATH
# kafka 2.8.0_2.12
export KAFKA_HOME=/opt/software/kafka_2.12-2.8.0
export PATH=$PATH:$KAFKA_HOME/bin
--------------------------------------------------------------------------

# 服务
jps -ml
--------------------------------------------------------------------------
# jdk
5248 sun.tools.jps.Jps -ml
# hadoop
2790 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager
2136 org.apache.hadoop.hdfs.server.namenode.NameNode
2920 org.apache.hadoop.yarn.server.nodemanager.NodeManager
2264 org.apache.hadoop.hdfs.server.datanode.DataNode
2456 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode
# hbase
3713 org.apache.hadoop.hbase.regionserver.HRegionServer start
3526 org.apache.hadoop.hbase.master.HMaster start
# zookeeper
3031 org.apache.zookeeper.server.quorum.QuorumPeerMain /opt/software/zookeeper-3.6.3/bin/../conf/zoo.cfg
# hive
9881 org.apache.hadoop.util.RunJar /opt/software/hive-3.1.2/lib/hive-metastore-3.1.2.jar org.apache.hadoop.hive.metastore.HiveMetaStore
9882 org.apache.hadoop.util.RunJar /opt/software/hive-3.1.2/lib/hive-service-3.1.2.jar org.apache.hive.service.server.HiveServer2
# dolphinscheduler
4206 org.apache.dolphinscheduler.server.log.LoggerServer
4403 org.apache.dolphinscheduler.api.ApiApplicationServer
4117 org.apache.dolphinscheduler.server.master.MasterServer
10314 org.apache.dolphinscheduler.server.worker.WorkerServer
10527 org.apache.dolphinscheduler.alert.AlertServer
# zeppelin
3993 org.apache.zeppelin.server.ZeppelinServer
--------------------------------------------------------------------------
```



## 四、数据采集

```shell
# 数据源为 csv 文件，考虑到技术栈的全面性做如下设计
	# 1、维度表：Customer, Store 通过存放于mysql中，通过spark来读取mysql并写至hive内。
	# 2、事实表：Transaction, Review
		# 一般做法有：
            # 2.1、通过 Flume 采集进入 Kafka
            # 2.2、通过 SparkStreaming 将数据采集存入 HBase
            # 2.3、通过 Hive 表映射 HBase 表
		# 此处由于数据完整，因此也采用flume先进入hdfs，然后通过读取hdfs数据进入hive中。
```

### 1）维度数据集采

#### 4.1、客户数据采集Customer

**维度表**【csv格式的数据】：将`customer.csv维度数据`保存至mysql中，然后通过`spark.read.jdbc(...)`读取数据至hive表中。

#### 4.2、店铺数据采集Store

**维度表**【csv格式的数据】：将`store.csv维度数据`保存至mysql中，然后通过`spark.read.jdbc(...)`读取数据至hive表中。

### 2）事实数据采集

```shell
# Flume的搭建
# 在项目中建立flume目录，用于数据采集
cd /root/project_ec/
mkdir flume
cd flume
mkdir store_flume transaction_flume
# 在各个主题的flume目录中构建四个基本目录
mkdir conf-file					# flume-ng 配置文件
mkdir position-file				# taildir source 采集断点续传记录目录
mkdir channel-checkpoint		# file channel 检查点目录
mkdir channel-data				# file channel 数据溢写目录
```

注意：**position-file目录**中创建一个文件**xxx_pos.log**，内部可先进行以下操作：

![](https://gitee.com/LemonWorld/picture/raw/master/img/flume%E6%93%8D%E4%BD%9C2.png)

#### 4.3、交易数据采集Transaction

**事实表**【json格式的数据】：在其对应的**conf-file目录**下创建**transaction.cnf**文件，将数据先传至于HDFS的目录中。

```shell
vim transaction.cnf
---------------------------------------------------------------------------
# 容器定义
a1.sources = r1
a1.channels = c1
a1.sinks = s1
# 数据源
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /root/project_ec/flume/transaction_flume/position-file/transaction_pos.log
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /root/project_ec/datasource/transaction_log/part-.*
a1.sources.r1.fileHeader = false
# 数据通道
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /root/project_ec/flume/transaction_flume/channel-checkpoint
a1.channels.c1.dataDirs = /root/project_ec/flume/transaction_flume/channel-data
# 数据池
a1.sinks.s1.type = hdfs
a1.sinks.s1.hdfs.fileType = DataStream
a1.sinks.s1.hdfs.writeFormat = Text
a1.sinks.s1.hdfs.path = hdfs://single:9000/external_commerce/transaction
a1.sinks.s1.hdfs.filePrefix = event-
a1.sinks.s1.hdfs.fileSuffix = .json
a1.sinks.s1.hdfs.rollInterval = 180
a1.sinks.s1.hdfs.rollSize = 134217728
a1.sinks.s1.hdfs.rollCount = 0
# 容器关联
a1.sinks.s1.channel = c1
a1.sources.r1.channels = c1
---------------------------------------------------------------------------

# hive sink : hive table must be transactional open ( ACID )
/opt/software/flume-1.9.0/bin/flume-ng agent \
--name a1 \
--conf /opt/software/flume-1.9.0/conf/ \
--conf-file /root/project_ec/flume/transaction_flume/conf-file/transaction.cnf \
-Dflume.root.logger=INFO,console
```

#### 4.4、评价数据采集Review

**事实表**【json格式的数据】：在其对应的**conf-file目录**下创建**review.cnf**文件，将数据先传至于HDFS的目录中。

```shell
vim review.cnf
---------------------------------------------------------------------------
# 容器定义
a1.sources = r1
a1.channels = c1
a1.sinks = s1
# 数据源
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /root/project_ec/flume/review_flume/position-file/review_pos.log
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /root/project_ec/datasource/review_log/part-.*
a1.sources.r1.fileHeader = false
# 数据通道
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /root/project_ec/flume/review_flume/channel-checkpoint
a1.channels.c1.dataDirs = /root/project_ec/flume/review_flume/channel-data
# 数据池
a1.sinks.s1.type = hdfs
a1.sinks.s1.hdfs.fileType = DataStream
a1.sinks.s1.hdfs.writeFormat = Text
a1.sinks.s1.hdfs.path = hdfs://single:9000/external_commerce/review
a1.sinks.s1.hdfs.filePrefix = event-
a1.sinks.s1.hdfs.fileSuffix = .json
a1.sinks.s1.hdfs.rollInterval = 180
a1.sinks.s1.hdfs.rollSize = 134217728
a1.sinks.s1.hdfs.rollCount = 0
# 容器关联
a1.sinks.s1.channel = c1
a1.sources.r1.channels = c1
---------------------------------------------------------------------------

# hive sink : hive table must be transactional open ( ACID )
/opt/software/flume-1.9.0/bin/flume-ng agent \
--name a1 \
--conf /opt/software/flume-1.9.0/conf/ \
--conf-file /root/project_ec/flume/review_flume/conf-file/review.cnf \
-Dflume.root.logger=INFO,console
```



## 五、近源层 ODS

### 1）共性代码

```scala
val logger = Logger.getLogger(ECOds.getClass)
def main(args: Array[String]): Unit = {
  import spark.implicits._
  import org.apache.spark.sql.functions._

  val spark: SparkSession = SparkFactory()
    .build()
    .baseConfig("ec_ods")
    .optimizeDriver()
    .optimizeExecutor()
    .optimizeLimit(maxLocalWaitS = 100)
    .optimizeSerializer()
    .optimizeNetAbout()
    .optimizeDynamicAllocation(true)
    .optimizeShuffle()
    .optimizeSpeculation(true)
    .optimizeRuntime(adaptiveEnabled = true, cboEnabled = true)
    .warehouseDir("hdfs://single:9000/hive312/warehouse")
    .end()

    val mysql: Getter = MysqlConfigFactory()
      .build()
      .setDriver("com.mysql.cj.jdbc.Driver")
      .setUrl("jdbc:mysql://single:3306/ebs?useUnicode=true&charSet=utf8") // 库名
      .setUser("zhou") // 远程用户
      .setPassword("Zhou@Remote0032") // 远程密码
      .finish()
    
    /**
        内容区域
    **/

    spark.stop()
    logger.info("EC ODS FINISHED")
}
```

### 2）建库 Create DataBase

```scala
/**
 * 创建近源层仓库(hive)：ec_ods
 */
spark.sql("create database if not exists ec_ods")
```

### 3）维度数据

> 维度数据入库：
>  * 分别从关系型数据库 mysql【single:3306】的ebs库中读取【维度表】 => [mysql数据导入hive中]
>  * 1、用户信息表 customer 写入 ec_ods.customer
>  * 2、店铺信息表 store 写入 ec_ods.store

#### 用户信息表 customer

```scala
spark
  .read
  .jdbc(mysql.getUrl, "customer", mysql.getConf)
  .write
  .mode(SaveMode.Overwrite) // Append是增加，Overwrite是覆盖
  .format("orc") // 存储数据格式
  .saveAsTable(s"ec_ods.customer") // 将mysql中表存储至hive中
```

#### 店铺表 store

```scala
spark
  .read
  .jdbc(mysql.getUrl, "store", mysql.getConf)
  .write
  .mode(SaveMode.Overwrite) // Append是增加，Overwrite是覆盖
  .format("orc") // 存储数据格式
  .saveAsTable(s"ec_ods.store") // 将mysql中表存储至hive中
```

### 4）事实数据

> 行为日志数据入库：
> * 分别 Flume 采集到 hdfs 的目录中external_commerce目录下的trasaction和review中读取数据，
> * 因为数据格式为 json 格式字符串，为了方便进一步清洗和扩展，直接将字段提取
> * 1、目录 transaction 中的数据写入数仓 ebs.transaction
> * 2、目录 review 中的数据写入数仓 ebs.review

#### 交易表 transaction

```scala
case class Line(json:String)

import org.apache.spark.sql.functions._
import spark.implicits._
val sc: SparkContext = spark.sparkContext
val rddLines1: RDD[Line] = sc
  .textFile("hdfs://single:9000/external_commerce/transaction", 4)
  .mapPartitions(_.map(line => Line(line)))
spark
  .createDataFrame(rddLines1)
  // json解析
  .select(
    json_tuple($"json", "transaction_id", "customer_id", "store_id", "price", "product", "date", "time")
      .as(Seq("transaction_id", "customer_id", "store_id", "price", "product", "date", "time"))
  )
  .write
  .format("orc")
  .mode(SaveMode.Overwrite)
  .saveAsTable("ec_ods.transaction")
```

#### 评价表 review

```scala
case class Line(json:String)

import org.apache.spark.sql.functions._
import spark.implicits._
val sc: SparkContext = spark.sparkContext
val rddLines2: RDD[Line] = sc
  .textFile("hdfs://single:9000/external_commerce/review", 4)
  .mapPartitions(_.map(line => Line(line)))
spark
  .createDataFrame(rddLines2)
  // json解析
  .select(
    json_tuple($"json", "transaction_id", "store_id", "review_score")
      .as(Seq("transaction_id", "store_id", "review_score"))
  )
  .write
  .format("orc")
  .mode(SaveMode.Overwrite)
  .saveAsTable("ec_ods.review")
```



## 六、明细层 DWD

### 6.1.1、业务分析 (清洗)

```shell
# 1、列裁剪: 轻聚层
	考虑到未来业务的扩展性
# 2、降维
	模型简单，交叉点少，无需降维
# 3、清洗
	客户表：
		语言数据存在错误
		脱敏：
			email 邮箱账号脱敏，平台保留：12345@qq.com => 1A2f4g@qq.com
			address 详细地址脱敏
			credit_no 信用卡号
	交易表：
		交易编号有重复
		日期不统一导致后期不方便对日期进行提取，计算等处理
			date:	2018/2/13
			time:	10:20 AM|PM
			timestamp : yyyy-MM-dd HH:mm:ss
		一天分成四段
			1、0~5
			2、6~11
			3、12~17
			4、18~23
	评价表：
		同一个交易编号在交易表中的店铺编号和评价表中的店铺编号不一致，以交易表为准
		评分残缺
```

### 6.1.2、共性代码

```scala
val logger = Logger.getLogger(ECDwd.getClass)
def main(args: Array[String]): Unit = {
  import spark.implicits._
  import org.apache.spark.sql.functions._

  val spark: SparkSession = SparkFactory()
    .build()
    .baseConfig("ec_dwd")
    .optimizeDriver()
    .optimizeExecutor()
    .optimizeLimit(maxLocalWaitS = 100)
    .optimizeSerializer()
    .optimizeNetAbout()
    .optimizeDynamicAllocation(true)
    .optimizeShuffle()
    .optimizeSpeculation(true)
    .optimizeRuntime(adaptiveEnabled = true, cboEnabled = true)
    .warehouseDir("hdfs://single:9000/hive312/warehouse")
    .end()

    /**
        内容区域
    **/

    spark.stop()
    logger.info("EC DWD FINISHED")
}
```

### 6.1.3、建库 Create DataBase

```scala
/**
 * 创建明细层仓库(hive)：ebs_dwd
 */
spark.sql("create database if not exists ec_dwd")
```

### 6.2、用户表创建与装载

```scala
/**
 * 维度表: 用户表加载 customer:
 * 1、需要根据【指标列】需求进行行列裁剪
 * 2、提取所需字段(指标列)：编号,信用卡，职位，性别，国家，邮箱域名
 */
spark
  .table("ec_ods.customer")
  .select($"customer_id", $"credit_no", $"job",
    $"gender", $"country", $"credit_type", $"last_name".as("customer_name"),
    regexp_extract($"email", ".*@(.*)", 1).as("email")
  )
  //存盘至hive的dwd数据库中
  .write
  .format("orc")
  .mode(SaveMode.Overwrite)
  .saveAsTable("ec_dwd.customer")
```

### 6.3、店铺表创建与装载

```scala
/**
 * 维度表: 商铺表加载 store
 * 1、字段全部保留
 * 2、提取所需字段(指标列)：商铺，商铺名称，员工数
 */
spark
  .table("ec_ods.store")
  .select($"store_id", $"store_name", $"employee_number"
  )
  .where($"store_id".isNotNull)
  //存盘至hive的dwd数据库中
  .write
  .format("orc")
  .mode(SaveMode.Overwrite)
  .saveAsTable("ec_dwd.store")
```

### 6.4、交易表创建与加载

```scala
/**
 * 事实表：交易表加载 transaction
 * 1、清洗重复的交易编号
 * 2、将日期和时间合并成标准格式 yyyy-MM-dd HH:mm:ss
 * 3、日期维度细化：年，季，月，周，日，时段(8)
 * 4、按年季月分区
 */
// 交易表transaction
// UDF:注册
spark.udf.register("my_date_format", (datetime: String) => dateFormat(datetime))
val tran: DataFrame = spark
  .table("ec_ods.transaction")
  // 转换类型
  .select(
    $"transaction_id".cast(IntegerType),
    $"customer_id".cast(IntegerType),
    $"store_id".cast(IntegerType),
    $"price".cast(DecimalType(10, 2)),
    $"product",
    // 2、将日期和时间合并成标准格式 yyyy-MM-dd HH:mm:ss
    callUDF("my_date_format", concat_ws(" ", $"date", $"time"))
      .as("tran_dt")
  )
  .cache()

// 所有的数据，开窗口：2为重复数据，1为不重复数据 => 窗口函数都是需要排序的order by
val allTran: DataFrame = tran
  .withColumn("rnk", // 排名
    row_number()
      .over(Window.partitionBy($"transaction_id").orderBy($"tran_dt"))
  ).cache()

// 有重复的数据
val tranRepeat: DataFrame = tran
  .groupBy($"transaction_id")
  .count()
  .where($"count" > 1)

// 不重复数据
val tranUnrepeated: Dataset[Row] = allTran
  .where($"rnk" === 1)
  .select(
    $"transaction_id",
    $"customer_id",
    $"store_id",
    $"price",
    $"product",
    $"tran_dt"
  )
  .cache()

// 在不重复数据中寻找最大的号
val maxTranId: DataFrame = tranUnrepeated
  .agg(max($"transaction_id").as("max_tran_id"))

// 1、清洗重复的交易编号
val tranRepeated: DataFrame = allTran
  // 重复的数据
  .where($"rnk" > 1)
  .withColumn("rn", // 行号
    row_number().over(Window.orderBy($"transaction_id")))
  .as("A")
  .crossJoin(maxTranId.as("M")) // 笛卡尔积
  // 最大值+行号:withColumn中同名列,覆盖原表字段
  //.withColumn("transaction_id",$"M.max_tran_id" + $"A.rn")
  .select(
    ($"M.max_tran_id" + $"A.rn").as("transaction_id"),
    $"customer_id",
    $"store_id",
    $"price",
    $"product",
    $"tran_dt"
  )

tranUnrepeated
  .unionAll(tranRepeated)
  // 3、日期维度细化：年，季，月，周(月周)，日(月日)，时段(8)
  .select(
    $"transaction_id",
    $"customer_id",
    $"store_id",
    $"price",
    $"product",
    $"tran_dt",
    year($"tran_dt").as("tran_year"),
    quarter($"tran_dt").as("tran_quarter"),
    month($"tran_dt").as("tran_month"),
    ceil(dayofmonth($"tran_dt") / 7).as("tran_month_week"), // 月周
    dayofmonth($"tran_dt").as("tran_day"),
    floor(hour($"tran_dt") / 3 + 1).as("tran_range") // 3小时一个时段
  )
  .repartition(1)
  .write
  .format("orc")
  .mode(SaveMode.Overwrite)
  // 4、按年季月分区
  .partitionBy("tran_year", "tran_quarter", "tran_month")
  .saveAsTable("ec_dwd.transaction")
```

### 6.5、评价表创建与装载

```scala
/**
 * 事实表：评价表加载 review
 * 1、同一个交易编号在交易表中的店铺号和评价表中的店铺号不一致，以交易表为准
 * 2、评价残缺
 */
// 评价表review
val tranDwd: DataFrame = spark.table("ec_dwd.transaction")
val review: DataFrame = spark
  .table("ec_ods.review")
  // 转换类型
  .select(
    $"transaction_id".cast(IntegerType),
    $"store_id".cast(IntegerType),
    $"review_score".cast(DecimalType(3, 2))
  )
  .as("R")
  //1、同一个交易编号在交易表中的店铺号和评价表中的店铺号不一致，以交易表为准
  .join(tranDwd.as("T"), Seq("transaction_id"), "left")
  .withColumn("alia_store_id",
    when($"R.store_id".isNull, $"R.store_id").otherwise($"T.store_id")
  )
  .select($"transaction_id", $"alia_store_id".as("store_id"), $"review_score")
  .cache()
// 平均评分
val avgReviewScore: DataFrame = review
  .where($"review_score".isNotNull)
  .agg(avg($"review_score").cast(DecimalType(3, 2)).as("avg_score"))
review
  .join(avgReviewScore)
  //2、评价残缺
  .withColumn("review_score",
    when($"review_score".isNull, $"avg_score")
      .otherwise($"review_score")
  )
  .select(
    $"transaction_id", $"store_id", $"review_score"
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_dwd.review")
```



## 七、轻聚层 DWS

### 7、准备工作

#### 1）业务分析 (聚合)

```shell
# 公共聚合方式与维度
	# 【日期维度表】
		日期维度表
	
	# 【客户表】
		1、信用卡类型、性别和国家、国家和性别和信用类型
            group by country,gender,credit_type
            grouping sets(credit_type,(gender,country),(country,gender,credit_type))
		
		2、工种
			count(*)
			group by job
			# 规则：工种名称为：a/b，取 b 作为工种	
		3、邮箱
			count(*)
			group by sub_mail
			# 规则：取邮箱域名作为邮箱种类
			substr(eamil,locate(emial,'@')-1)
			substring_index(email,'@',-1)
			regexp_extract(email,'(.*)@.*',1)
	
	# 【交易表】
	  	1、单日期维度
            count(*),sum(price)
            group by
                tran_month,
                tran_season,
                tran_month,
                tran_day,
                tran_section
            grouping sets(
                dim_year,
                (dim_year,dim_season),
                (dim_year,dim_month,dim_day),
                (dim_year,dim_month,dim_day,dim_section)
            ) 
           
	  	2、客户维度
            count(*),sum(price)
            group by dim_year,dim_month,customer_id
            grouping sets(
                customer_id,
                (dim_year,dim_month,customer_id)
            }
	  
	  	3、国家维度
            sum(price)
            group by country
		
		4、产品维度
            sum(price),count(*),size(collect_set(customer_id))
            group by product
        	
		
		5、季、周度独立用户
            size(collect_set(T.customer_id))
            group by
              dim_year,
              dim_season,
              dim_month,
              dim_month_week
            grouping sets(
               (D.dim_year,D.dim_season,D.dim_month,D.dim_month_week)
               (D.dim_year,D.dim_season)
            )
	
    # 【店铺维度】
      	1、交易数、交易额、店铺员工客户比
            count(*), 	# 客流
            sum(price), # 消费额度
            employee_number/size(collect_set(customer_id))	# 员工与独立客户比
            group by store_id
      		
		2、交易情况：
			交易额+独立客户数
			sum(price)+size(collect_set(customer_id))
			# 交易额+独立客户数+年周销售额TOP3次数+年周平均独立客户数+月均新增独立客户数+均评分
			group by store_id
		
		3、店铺人员顾客比
			agg(size(collect_set("customer_id")).as("uq_customer_cnt"))
		
		4、根据各店铺、产品购买的唯一客户数锁定各店铺最受欢迎的产品
			size(collect_set(customer_id))
			group by store_id, product
			
		5、年月店铺收入
			sum(price)
			group by store_id,dim_year,dim_month	
		
		6、店铺按时段算流量
			count(*)
			group by store_id, tran_section
		
		7、忠实粉丝
			# 每年3个月以上在该店铺有交易
			# 总交易数的30%或交易额的30%以上发生在该店铺
			store_id, customer_id		count(*) as sc_cnt
        	customer_id 				count(*) as c_cnt
        	store_id, customer_id		sc_cnt/c_cnt as ratio
        		where c_cnt>=3 ratio>0.5
        	group by store_id, customer_id
       
  	# 【评价表】
  		1、客户评价覆盖率：评价订单数/总订单数
  		
  		2、总评分分布情况，店铺评分分布情况
  		count(transaction_id)
  		group by store_id,review_score
  		grouping sets(
  			review_score,
  			(store_id,review_score)
  		)
  		
  		3、客户好评店铺分布
  		count(max_score_in_store)/count(max_score)>=0.67
  		group by customer_id，store_id
  		
  		4、店铺的评分分布
  		select($"store_name", $"L.review_score", $"review_score_cnt")
  		group by store_id,review_score
```

#### 2）建库 Create DataBase

```scala
/**
 * 创建数仓轻聚层库(hive)：ec_des
 */
spark.sql("create database if not exists ec_dws")
```

#### 3）共性代码

```scala
val logger = Logger.getLogger(ECDws.getClass)
def main(args: Array[String]): Unit = {
  import spark.implicits._
  import org.apache.spark.sql.functions._

  val spark: SparkSession = SparkFactory()
    .build()
    .baseConfig("ec_dws")
    .optimizeDriver()
    .optimizeExecutor()
    .optimizeLimit(maxLocalWaitS = 100)
    .optimizeSerializer()
    .optimizeNetAbout()
    .optimizeDynamicAllocation(true)
    .optimizeShuffle()
    .optimizeSpeculation(true)
    .optimizeRuntime(adaptiveEnabled = true, cboEnabled = true)
    .warehouseDir("hdfs://single:9000/hive312/warehouse")
    .end()

    /**
        内容区域
    **/

    spark.stop()
    logger.info("EC DWS FINISHED")
}
```

### 7.1、日期维度表

```scala
// 日期维度表
import core.Validator.dimDate
val rowDt: Row = spark.table("ec_dwd.transaction")
  .agg(
    date_add(max(to_date($"tran_dt")), 1).as("max_date"),
    min(to_date($"tran_dt")).as("min_date")
  )
  .take(1)(0)
val minDate: String = rowDt.getAs[Date]("min_date").toString
val maxDate: String = rowDt.getAs[Date]("max_date").toString
spark
  .createDataFrame(dimDate(minDate, maxDate))
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.dim_date")
```

### 7.2、客户表(国家、性别、信用卡类型)

```scala
spark.sql(
    """
      |select
      | grouping__id as gid,country,gender,credit_type,count(customer_id) as agg_cnt
      |from ec_dwd.customer
      |group by country,gender,credit_type
      |grouping sets(credit_type,(gender,country),(country,gender,credit_type))
      |""".stripMargin)
  .repartition(1)
  .write
  .format("orc")
  .mode(SaveMode.Overwrite)
  .partitionBy("gid")
  .saveAsTable("ec_dws.customer_by_cgc")

/*	维度字典
    --分组字段				 		 分组编号
    credit_type 					6
    gender,country					1
    country,gender,credit_type		0
*/
```

### 7.3、客户表(工种)

```scala
spark
  .table("ec_dwd.customer")
  .groupBy("job")
  .agg(count("customer_id").as("job_cnt"))
  .repartition(1)
  .write
  .format("orc")
  .mode(SaveMode.Overwrite)
  .saveAsTable("ec_dws.customer_by_job")
```

### 7.4、客户表(邮箱)

```scala
spark
  .table("ec_dwd.customer")
  .groupBy("email")
  .agg(count("customer_id").as("email_cnt"))
  .repartition(1)
  .write
  .format("orc")
  .mode(SaveMode.Overwrite)
  .saveAsTable("ec_dws.customer_by_email")
```

### 7.5、交易表(日期)

```scala
val date: DataFrame = spark.table("ec_dws.dim_date")
spark
  .table("ec_dwd.transaction")
  .as("tran")
  .join(date.as("dim"),
    Seq("tran_year", "tran_quarter", "tran_month", "tran_month_week", "tran_day", "tran_range"),
    "right"
  )
  .rollup("tran_year", "tran_quarter", "tran_month", "tran_month_week", "tran_day", "tran_range")
  .agg(
    grouping_id().as("gid"),
    sum($"price").cast(DecimalType(10, 2)).as("sum_amount"),
    avg($"price").cast(DecimalType(10, 2)).as("avg_amount"),
    count($"price").as("tran_count") // 交易数
  )
  // 再次清洗：去除null,用withColumn覆盖
  .withColumn("sum_amount",
    when($"sum_amount".isNull, 0).otherwise($"sum_amount")
  )
  .withColumn("avg_amount",
    when($"avg_amount".isNull, 0).otherwise($"avg_amount")
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.transaction_by_dim_date")


// 维度字典
分组编号					分组字段
0		"tran_year","tran_quarter","tran_month","tran_month_week","tran_day","tran_range"
1		"tran_year","tran_quarter","tran_month","tran_month_week","tran_day"
3		"tran_year","tran_quarter","tran_month","tran_month_week"
7		"tran_year","tran_quarter","tran_month"
15		"tran_year","tran_quarter"
31		"tran_year"
63
```

### 7.6、交易表(顾客)

```scala
// 交易、客户、日期客户
spark.sql(
    """
      |with dim_year_month as(
      | select
      |   tran_year,tran_month
      | from ec_dws.dim_date
      | group by tran_year,tran_month
      |),
      |dim_year_month_customer as(
      | select
      |   D.tran_year,D.tran_month,C.customer_id
      | from ec_dwd.customer as C
      | cross join dim_year_month as D
      |)
      |select
      |   grouping__id as gid,
      |   tran_year,tran_month,customer_id,
      |   cast(sum(price) as decimal(10,2)) as sum_amount,
      |   cast(avg(price) as decimal(10,2)) as avg_amount,
      |   count(price) as tran_count
      |from ec_dwd.transaction
      |right join dim_year_month_customer
      |using(tran_year,tran_month,customer_id)
      |group by tran_year,tran_month,customer_id
      |grouping sets(customer_id,(tran_year,tran_month,customer_id))
      |""".stripMargin)
  // 再次清洗：去除null,用withColumn覆盖
  .withColumn("sum_amount",
    when($"sum_amount".isNull, 0).otherwise($"sum_amount")
  )
  .withColumn("avg_amount",
    when($"avg_amount".isNull, 0).otherwise($"avg_amount")
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .partitionBy("tran_year", "tran_month")
  .saveAsTable("ec_dws.transaction_by_ymc")

// 维度字典
分组编号					分组字段
 0				   dim_year,dim_month,customer_id
 6				   customer_id
```

### 7.7、交易表(产品)

```scala
val avgMonthCount: DataFrame = spark.table("ec_dwd.transaction")
  .groupBy("product", "tran_month")
  .agg(count("transaction_id").as("month_count"))
  .groupBy("product")
  .agg(avg("month_count").as("avg_month_count"))
spark.table("ec_dwd.transaction")
  .groupBy("product")
  .agg(
    sum("price").cast(DecimalType(10, 2)).as("sum_amount"),
    size(collect_set("customer_id")).as("uq_customer_count"), // 客户总数
    count("transaction_id").as("tran_count") // 订单数
  )
  .join(avgMonthCount, Seq("product"), "inner")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.transaction_product")
```

### 7.8、交易表(国家)

```scala
val customer: DataFrame = spark
  .table("ec_dwd.customer")
// 广播
val bcDF: Broadcast[DataFrame] = spark.sparkContext.broadcast(customer)
spark
  .table("ec_dwd.transaction").as("T")
  // 获取广播出去的小表
  .join(bcDF.value.as("C"), Seq("customer_id"), "inner")
  .groupBy("C.country")
  .agg(
    sum("T.price").cast(DecimalType(10, 2)).as("sum_amount"),
    avg("T.price").cast(DecimalType(10, 2)).as("avg_amount"),
    count($"T.customer_id").as("tran_count"), // 交易数
    size(collect_set("customer_id")).as("uq_customer_count") // 唯一客户数
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.transaction_by_country")
```

### 7.9、独立用户(季度&周)

```scala
spark.table("ec_dwd.transaction")
  .rollup("tran_year", "tran_quarter", "tran_month", "tran_month_week")
  .agg(
    grouping_id().as("gid"),
    size(collect_set("customer_id")).as("uq_customer_count")
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.uq_customer_count_by_qw")

/**
 * 店铺 store
 */
// 交易表(店铺)
spark.table("ec_dwd.transaction")
  .groupBy("store_id")
  .agg(
    count("transaction_id").as("customer_count"),
    sum("price").as("sum_amount")
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.transaction_by_store")

// 维度字典
分组编号					分组字段
 0				   		所有数据【周独立用户数】
 3				   		季度独立用户数
```

### 7.10、交易表(店铺)

```scala
spark.table("ec_dwd.transaction")
  .groupBy("store_id")
  .agg(
    count("transaction_id").as("customer_count"),
    sum("price").as("sum_amount")
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.transaction_by_store")
```

### 7.11、交易表(店铺交易情况)

```scala
/*
  店铺交易情况 = 交易额+独立客户数+年周销售额TOP3次数+年周平均独立客户数+月均新增独立客户数+均评分
  一般思想：归一化 => 先求出各个条件中的最大值，然后套入公式：(每列值/列中最大值)*权重
 */
// 1、年周平均独立客户数
val weekAvg: DataFrame = spark.table("ec_dwd.transaction")
  .groupBy("store_id", "tran_year", "tran_month", "tran_month_week")
  .agg(
    size(collect_set("customer_id")).as("week_uq_customer_cnt") // 周进行汇总
  )
  .groupBy("store_id") // 均值
  .agg(avg("week_uq_customer_cnt").as("week_avg_uq_counter_cnt"))
// 2、月均新增购买的独立客户数【一般思路：新增用户=>差集】
// 此处另一种思路
val monthNewAdd: DataFrame = spark.table("ec_dwd.transaction")
  // 最早购买并排序
  .withColumn("rn", row_number().over(Window.partitionBy("customer_id").orderBy("tran_dt")))
  .where($"rn" === 1) // 选取第一条数据（用户第一次购买）
  .groupBy("store_id", "tran_month")
  .agg(count("customer_id").as("month_add_customer_cnt"))
  .groupBy("store_id")
  .agg(avg("month_add_customer_cnt").as("avg_month_add_customer_cnt"))
// 3、均评分
val avgReview: DataFrame = spark
  .table("ec_dwd.review")
  .groupBy("store_id")
  .agg(avg("review_score").as("avg_review_score"))
// 4、年周销售额TOP3次数
val weekTop3: DataFrame = spark.table("ec_dwd.transaction")
  .groupBy("store_id", "tran_year", "tran_month", "tran_month_week")
  .agg(sum("price").as("sum_amount"))
  // 先开窗口进行排名，再筛选前三
  .withColumn("week_rnk",
    dense_rank()
      .over(Window.partitionBy("tran_year", "tran_month", "tran_month_week")
        .orderBy($"sum_amount".desc)
      ))
  .where($"week_rnk" <= 3)
  .groupBy("store_id")
  .agg(count("week_rnk").as("week_top3_count"))
// 汇总：六个指标的汇总
val all = spark.table("ec_dwd.transaction")
  .groupBy("store_id")
  .agg(
    sum("price").as("sum_amount"), // 交易额
    size(collect_set("customer_id")).as("total_uq_customer_cnt"), // 总的独立客户数（一共有多少买过东西的客户）
  )
  .join(weekAvg, "store_id")
  .join(monthNewAdd, "store_id")
  .join(avgReview, "store_id")
  .join(weekTop3, "store_id")
  .cache()
// 归一化操作
val allMax = all
  .agg(
    // 先求出六个指标中的最大值
    max("sum_amount").as("max_sum_amount"),
    max("total_uq_customer_cnt").as("max_uq_cust_cnt"),
    max("week_avg_uq_counter_cnt").as("max_week_avg_uq_cust_cnt"),
    max("avg_month_add_customer_cnt").as("max_month_add_cust_cnt"),
    max("avg_review_score").as("max_avg_review_score"),
    max("week_top3_count").as("max_week_top3_cnt"),
  )
// 综合指标【1以内小数】
// (每个值/该列最大值)*权重【权重自己定】
all.as("A").crossJoin(allMax.as("M"))
  .select($"store_id",
    (
      (($"A.sum_amount" / $"M.max_sum_amount") * 0.1).cast(DecimalType(5, 4)) +
        (($"A.total_uq_customer_cnt" / $"M.max_uq_cust_cnt") * 0.1).cast(DecimalType(5, 4)) +
        (($"A.week_top3_count" / $"M.max_week_top3_cnt") * 0.3).cast(DecimalType(5, 4)) +
        (($"A.week_avg_uq_counter_cnt" / $"M.max_week_avg_uq_cust_cnt") * 0.1).cast(DecimalType(5, 4)) +
        (($"A.avg_month_add_customer_cnt" / $"M.max_month_add_cust_cnt") * 0.3).cast(DecimalType(5, 4)) +
        (($"A.avg_review_score" / $"M.max_avg_review_score") * 0.1).cast(DecimalType(5, 4))
      ).as("sale_factor") // 店铺销售质量指数
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  // 店铺交易综合指标
  .saveAsTable("ec_dws.store_tran_factor")
```

### 7.12、交易表(店铺产品，唯一客户数)

```scala
// 店铺产品，唯一客户数量
spark.table("ec_dwd.transaction")
  .groupBy("store_id", "product")
  .agg(size(collect_set("customer_id")).as("uq_customer_cnt"))
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.pop_product_of_store")
```

### 7.13、交易表(店铺员工顾客比)

```scala
val storeUqCust: DataFrame = spark.table("ec_dwd.transaction")
  .groupBy("store_id")
  .agg(size(collect_set("customer_id")).as("uq_customer_cnt"))
spark
  .table("ebs_dwd.store")
  .join(storeUqCust, "store_id")
  .select($"store_id", ($"employee_number" / $"uq_customer_cnt").as("emp_cus_ratio"))
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.store_emp_cus_ration")
```

### 7.14、交易(店铺年月销售额)

```scala
spark.table("ec_dwd.transaction")
  .groupBy("store_id", "tran_year", "tran_month")
  .agg(sum("price").as("sum_amount"))
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.store_year_month_sum_amount")
```

### 7.15、交易(店铺时段流量)

```scala
spark.table("ec_dwd.transaction")
  .groupBy("store_id", "tran_range")
  .agg(count("transaction_id").as("range_flow"))
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.store_range_flow")
```

### 7.16、交易(店铺忠实粉丝)

```scala
/* 店铺的忠实粉丝：
    标准：花钱额度，购买次数，月均购买次数，近3个月的额度，近3个月的次数，近3周的额度，近3周的次数
    归一化 => 先求出各个条件中的最大值，然后套入公式：(每列值/列中最大值)*权重
*/
// 店铺客户月均
val storeCusMonthAvg: DataFrame = spark.table("ec_dwd.transaction")
  .groupBy("store_id", "customer_id", "tran_year", "tran_month")
  .agg(
    sum("price").as("sum_amount"),
    count("transaction_id").as("sum_count")
  )
  .groupBy("store_id", "customer_id")
  .agg(
    avg("sum_amount").cast(DecimalType(10, 2)).as("month_avg_amount"),
    avg("sum_count").cast(DecimalType(10, 2)).as("month_avg_count")
  )
val tranTmp: DataFrame = spark.table("ec_dwd.transaction")
  .withColumn("max_date", max($"tran_dt").over(Window.orderBy($"tran_dt".desc)))
  .cache()
// 近三个月
val recent3Month: DataFrame = tranTmp
  .where(datediff($"max_date", $"tran_dt") <= 90) // 近三个月
  .groupBy("store_id", "customer_id")
  .agg(
    sum("price").cast(DecimalType(10, 2)).as("sum_amount_recent_3_month"),
    avg("transaction_id").cast(DecimalType(10, 2)).as("sum_count_recent_3_month")
  )
// 近三周
val recent3Week: DataFrame = tranTmp
  .where(datediff($"max_date", $"tran_dt") <= 21) // 近三个月
  .groupBy("store_id", "customer_id")
  .agg(
    sum("price").cast(DecimalType(10, 2)).as("sum_amount_recent_3_week"),
    avg("transaction_id").cast(DecimalType(10, 2)).as("sum_count_recent_3_week")
  )
val storeCusTmp: DataFrame = spark.table("ec_dwd.transaction")
  .groupBy("store_id", "customer_id")
  .agg(
    sum("price").as("sum_amount"), //花钱额度
    count("transaction_id").as("sum_count") //购买次数
  )
  .join(storeCusMonthAvg, Seq("store_id", "customer_id"))
  .join(recent3Month, Seq("store_id", "customer_id"))
  .join(recent3Week, Seq("store_id", "customer_id"))
  .cache()
// 综合指标【1以内小数】
// (每个值/该列最大值)*权重【权重自己定】
val storeCusMax: DataFrame = storeCusTmp
  .agg(
    // 先求出六个指标中的最大值
    max("sum_amount").as("max_sum_amount"),
    max("sum_count").as("max_sum_count"),
    max("month_avg_amount").as("max_month_avg_amount"),
    max("month_avg_count").as("max_month_avg_count"),
    max("sum_amount_recent_3_month").as("max_sum_amount_recent_3_month"),
    max("sum_count_recent_3_month").as("max_sum_count_recent_3_month"),
    max("sum_amount_recent_3_week").as("max_sum_amount_recent_3_week"),
    max("sum_count_recent_3_week").as("max_sum_count_recent_3_week"),
  )
// 客户忠诚指数
storeCusTmp
  .crossJoin(storeCusMax)
  .select($"store_id", $"customer_id",
    (
      ($"sum_amount" / $"max_sum_amount") * 0.07 +
        ($"sum_count" / $"max_sum_count") * 0.07 +
        ($"month_avg_amount" / $"max_month_avg_amount") * 0.08 +
        ($"month_avg_count" / $"max_month_avg_count") * 0.08 +
        ($"sum_amount_recent_3_month" / $"max_sum_amount_recent_3_month") * 0.15 +
        ($"sum_count_recent_3_month" / $"max_sum_count_recent_3_month") * 0.15 +
        ($"sum_amount_recent_3_week" / $"max_sum_amount_recent_3_week") * 0.2 +
        ($"sum_count_recent_3_week" / $"max_sum_count_recent_3_week") * 0.2
      ).cast(DecimalType(5, 4)).as("faith_factor")
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.store_faith_factor")
```

### 7.17、评价(覆盖率)

```scala
val review: DataFrame = spark.table("ec_dwd.review").cache()
spark.table("ec_dwd.transaction")
  .join(review, Seq("transaction_id"), "left")
  .agg(
    (count("review_score") / count("transaction_id"))
      .as("review_coverage")
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.review_coverage")
```

### 7.18、评分(分布)

```scala
spark.table("ec_dwd.review")
  .groupBy("review_score")
  .agg(count("*").as("review_distribution")) // 不同评分的总人数
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.review_distribution")
```

### 7.19、评价(客户好评店铺分布)

```scala
// 客户好评店铺分布：客户给出的最高评分是否在同一个店铺【总体】
val cusStoreReview: DataFrame = spark.table("ec_dwd.review")
  .where($"review_score" >= 4) // 好评
  .as("R")
  .join(spark.table("ec_dwd.transaction").as("T"), "transaction_id")
  .groupBy("T.customer_id", "R.store_id")
  .agg(count("transaction_id").as("cus_store_good_cnt"))
  .cache()
val cusReview: DataFrame = cusStoreReview
  .groupBy("customer_id")
  .agg(sum("cus_store_good_cnt").as("cus_good_cnt"))
  .where($"cus_good_cnt" > 3)
cusStoreReview
  .join(cusReview, "customer_id")
  .select(
    $"customer_id", $"store_id",
    ($"cus_store_good_cnt" / $"cus_good_cnt").cast(DecimalType(3, 2)).as("cus_store_good_rate") // 好评覆盖率
  )
  .where($"cus_store_good_rate" >= 0.67)
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.cus_good_review_distribution_of_store")
```

### 7.20、评价(店铺的评分分布)

```scala
case class Review(review_score:Float) // 样例类

val frmScores: DataFrame = spark
  .createDataFrame(Seq(Review(1), Review(2), Review(3), Review(4), Review(5)))
  .select($"review_score".cast(DecimalType(3, 2))) // 所有关于评分数据
val frmStoreScores: DataFrame = spark.table("ec_dwd.review")
  .groupBy("store_id", "review_score")
  .agg(count("*").as("review_score_cnt")) // 店铺评分数量
spark.table("ec_dwd.store")
  .crossJoin(frmScores)
  .as("L")
  .join(frmStoreScores.as("R"), Seq("store_id", "review_score"), "left")
  .withColumn(
    "review_score_cnt",
    when($"review_score_cnt".isNull, 0).otherwise($"review_score_cnt")
  )
  .select($"store_name", $"L.review_score", $"review_score_cnt")
  .orderBy("store_name", "L.review_score")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc") // 列式存储
  .saveAsTable("ec_dws.review_distribution_of_store")
```



## 八、应用层 ADS

【**注意**】：直接面向大屏所展示的数据进行编写。

### 8、准备工作

#### 1）建库 Create DataBase

```scala
/**
 * 创建应用层仓库(hive)：ebs_ads_report
 */
spark.sql("create database if not exists ec_ads_report")
```

#### 2）共性代码

```scala
val logger = Logger.getLogger(ECAdsFR.getClass)
def main(args: Array[String]): Unit = {
  import spark.implicits._
  import org.apache.spark.sql.functions._

  val spark: SparkSession = SparkFactory()
    .build()
    .baseConfig("ec_ads")
    .optimizeDriver()
    .optimizeExecutor()
    .optimizeLimit(maxLocalWaitS = 100)
    .optimizeSerializer()
    .optimizeNetAbout()
    .optimizeDynamicAllocation(true)
    .optimizeShuffle()
    .optimizeSpeculation(true)
    .optimizeRuntime(adaptiveEnabled = true, cboEnabled = true)
    .warehouseDir("hdfs://single:9000/hive312/warehouse")
    .end()

    // 共享数据
    // 维度数据 => dwd
    val frmCustomer: DataFrame = spark.table("ec_dwd.customer").cache()
    val frmStore: DataFrame = spark.table("ec_dwd.store").cache()
    // 将广播变量（维度数据）广播出去
    val bcCustomer: Broadcast[DataFrame] = spark.sparkContext.broadcast(frmCustomer)
    val bcStore: Broadcast[DataFrame] = spark.sparkContext.broadcast(frmStore)

    /**
        内容区域
    **/

    spark.stop()
    logger.info("EC ADS FINISHED")
}
```

### 8.1、客户分析

#### 8.1.1、各类交易汇总

```scala
// 大屏四处的显示数据：一行四列（总人数，中国客户数，美国客户数，加拿大客户数）【按照列绑定数据】
spark.table("ec_dwd.customer")
  .groupBy("country")
  .agg(count("*").as("country_cnt"))
  // 行转列(=> 一行四列)
  .agg(
    sum("country_cnt").as("all_cnt"),
    sum(when($"country" === "China", $"country_cnt").otherwise(0)).as("china_cnt"),
    sum(when($"country" === "United States", $"country_cnt").otherwise(0)).as("usa_cnt"),
    sum(when($"country" === "Canada", $"country_cnt").otherwise(0)).as("canada_cnt")
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.customer_singles")
```

#### 8.1.2、销售额占比

```scala
spark.table("ec_dws.transaction_by_country")
  .select("country", "sum_amount")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.distribution_by_sum_amount")
```

#### 8.1.3、销售最多的前10名客户

```scala
spark.table("ec_dws.transaction_by_ymc")
  .where($"gid" === 6)
  .select("customer_id", "sum_amount")
  .withColumn("rnk", dense_rank().over(Window.orderBy($"sum_amount".desc)))
  .where($"rnk" <= 10)
  .join(bcCustomer.value, "customer_id") // 用户名
  .select("customer_name", "sum_amount", "rnk")
  .orderBy($"rnk")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.customer_top10_by_sum_amount")
```

#### 8.1.4、客户常用信用卡

```scala
spark.table("ec_dws.customer_by_cgc")
  .where($"gid" === 6)
  .withColumn("rnk", dense_rank().over(Window.orderBy($"agg_cnt".desc)))
  .select("credit_type", "rnk")
  .orderBy("rnk")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.credit_type")
```

#### 8.1.5、季度独立客户数

```scala
spark.table("ec_dws.uq_customer_count_by_qw")
  .where($"gid" === 3)
  .orderBy()
  .select(concat_ws("_",
    $"tran_year".cast(StringType),
    $"tran_quarter".cast(StringType)
  ).as("year_quarter"),
    $"uq_customer_count"
  )
  .orderBy("year_quarter")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.uq_customer_cnt_by_year_quarter")
```

#### 8.1.6、热点工作岗位(Top5)

```scala
spark.table("ec_dws.customer_by_job")
  .withColumn("rnk", dense_rank().over(Window.orderBy($"job_cnt".desc)))
  .where($"rnk" <= 5)
  .orderBy("rnk")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.job_top5")
```

#### 8.1.7、销售额前10的客户占比

```scala
// 找出【交易量】最大的10个【客户】(【前十】)
spark.table("ec_dws.transaction_by_ymc")
  .where($"gid" === 6)
  .select("customer_id", "tran_count")
  .withColumn("rnk", dense_rank().over(Window.orderBy($"tran_count".desc)))
  .where($"rnk" <= 10)
  .join(bcCustomer.value, "customer_id") // 用户名
  .select("customer_name", "tran_count", "rnk")
  .orderBy($"rnk")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.customer_top10_by_tran_count")
```

### 8.2、店铺统计

#### 8.2.1、店铺交易情况

```scala
// 交易情况前三名: Lablaws,Walmart,FoodMart
spark.table("ec_dws.store_tran_factor")
  .withColumn("rnk", dense_rank().over(Window.orderBy($"sale_factor".desc)))
  .where($"rnk" <= 3)
  .join(bcStore.value, "store_id")
  .select("store_name", "sale_factor", "rnk")
  // 大屏三处展示数据【按照列绑定数据】
  .agg(
    sum(when($"store_name" === "Lablaws", $"sale_factor").otherwise("0")).as("lablaws_sale_factor"),
    sum(when($"store_name" === "Walmart", $"sale_factor").otherwise("0")).as("walmart_sale_factor"),
    sum(when($"store_name" === "FoodMart", $"sale_factor").otherwise("0")).as("foodmart_sale_factor"),
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.store_singles")
```

#### 8.2.2、店铺评价5分分布

```scala
spark.table("ec_dws.review_distribution_of_store")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.review5_distribution_store")
```

#### 8.2.3、评分分布

```scala
spark.table("ec_dws.review_distribution_of_store")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.review_distribution_of_store")
```

#### 8.2.4、月均销量热点商品

```scala
spark.table("ec_dws.transaction_product")
  .withColumn("rnk", dense_rank().over(Window.orderBy($"avg_month_count".desc))) // 总价
  .where($"rnk" <= 5)
  .select("product", "avg_month_count", "rnk")
  .orderBy("rnk")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.product_top5_by_avg_month_sale_cnt")
```

#### 8.2.5、店铺热点商品排名

```scala
// 根据客户数量找出最受欢迎的5种产品
spark.table("ec_dws.transaction_product")
  .withColumn("rnk", dense_rank().over(Window.orderBy($"uq_customer_count".desc))) // 总价
  .where($"rnk" <= 5)
  .select("product", "uq_customer_count", "rnk")
  .orderBy("rnk")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.rank_of_hot_product")
```

#### 8.2.6、2018月度环比

```scala
// 按年(只有2018数据)和月计算每家店的收入的环比(关联维度数据，保证数据完整型)
val frmStoreYearMonth: DataFrame = spark
  .table("ec_dws.store_year_month_sum_amount")
  .cache()
spark.table("ec_dws.dim_date")
  .where($"tran_year" === 2018) // 锁定为2018，否则以目前的数据而言，会出错的
  .groupBy("tran_year", "tran_month")
  .count() // 到此处，为了防止数据有缺失
  .crossJoin(bcStore.value)
  .join(frmStoreYearMonth, Seq("store_id", "tran_year", "tran_month"), "left")
  // 环比
  .withColumn("lag_sum_amount",
    lag("sum_amount", 1).over(Window.partitionBy("store_id").orderBy("tran_month"))
  )
  .where($"tran_month" =!= 1)
  .select($"store_name",
    concat($"tran_month".cast(StringType), lit("/"), ($"tran_month" - 1).cast(StringType)).as("chain_name"),
    ($"sum_amount" - $"lag_sum_amount").as("chain_value")
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.store_2018_month_chain")
```

### 8.3、交易情况

#### 8.3.1、各类交易汇总

```scala
// 各类交易汇总（国内外销售情况+销售总额+销售总数+男女占比）
// 大屏八处数据展示：交易情况(国内国外对比)【按照列绑定数据】
// 国内外总额，总销量，总客户数
// 销售总额，总销量
spark.table("ec_dws.transaction_by_country")
  // 国内外
  .withColumn("country", when($"country" === "China", "domestic").otherwise("foreign"))
  .groupBy("country")
  .agg(
    sum("sum_amount").as("sum_amount"), // 总额
    sum("tran_count").as("tran_count"), // 销量
    sum("uq_customer_count").as("uq_customer_count") // 唯一客户数
  )
  .agg(
    sum("sum_amount").as("all_amount"),
    sum("tran_count").as("all_count"),
    sum(when($"country" === "domestic", $"sum_amount").otherwise(0)).as("domestic_amount"), //国内总额
    sum(when($"country" === "foreign", $"sum_amount").otherwise(0)).as("foreign_amount"), //国外总额
    sum(when($"country" === "domestic", $"tran_count").otherwise(0)).as("domestic_count"), //国内总量
    sum(when($"country" === "foreign", $"tran_count").otherwise(0)).as("foreign_count"), //国外总量
    sum(when($"country" === "domestic", $"uq_customer_count").otherwise(0)).as("domestic_uq_customer_count"), //国内客户数
    sum(when($"country" === "foreign", $"uq_customer_count").otherwise(0)).as("foreign_uq_customer_count") //国外客户数
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.transaction_singles_one")
// 男客户数，女客户数
spark.table("ec_dws.customer_by_cgc")
  .where($"gid" === 1)
  .groupBy("gender")
  .agg(
    sum("agg_cnt").as("customer_count")
  )
  .agg(
    sum(when($"gender" === "Male", $"customer_count").otherwise(0)).as("male_customer_count"),
    sum(when($"gender" === "Female", $"customer_count").otherwise(0)).as("female_customer_count"),
  )
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.transaction_singles_two")
```

#### 8.3.2、国家销售总额世界占比

```scala
spark.table("ec_dws.transaction_by_country")
  .select("country", "sum_amount")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.country_distribution_by_sum_amount")
```

#### 8.3.3、商品销售额Top5

```scala
// 按总价找出最受欢迎的5种产品
spark.table("ec_dws.transaction_product")
  .withColumn("rnk", dense_rank().over(Window.orderBy($"sum_amount".desc))) // 总价
  .where($"rnk" <= 5)
  .select("product", "sum_amount", "rnk")
  .orderBy("rnk")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.product_top5_by_sum_amount")
```

#### 8.3.4、时段交易额

```scala
// 按时间段计算总收入
spark.table("ec_dws.transaction_by_dim_date")
  .where($"gid" === 0)
  .groupBy("tran_range")
  .agg(sum("sum_amount").as("sum_amount"))
  .orderBy("tran_range")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.sum_amount_by_range")
```

#### 8.3.5、热销产品

```scala
spark.table("ec_dws.transaction_product")
  .repartition(1)
  .write
  .format("orc")
  .mode(SaveMode.Overwrite)
  .saveAsTable("ec_ads_report.tran_distribution_by_product")
```

#### 8.3.6、季度销售额

```scala
// 每个季度的总收入
spark.table("ec_dws.transaction_by_dim_date")
  .where($"gid" === 15)
  .select("tran_year", "tran_quarter", "sum_amount")
  .orderBy("tran_year", "tran_quarter")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.sum_amount_by_year_quarter")
```

#### 8.3.7、商品销售额前三

```scala
// 按总价找出最受欢迎的3种产品
spark.table("ec_dws.transaction_product")
  .withColumn("rnk", dense_rank().over(Window.orderBy($"sum_amount".desc))) // 总价
  .where($"rnk" <= 3)
  .select("product", "sum_amount", "rnk")
  .orderBy("rnk")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.product_top3_by_sum_amount")
```

#### 8.3.8、产品独立客户数

```scala
// 根据客户数量找出最受欢迎的5种产品
spark.table("ec_dws.transaction_product")
  .withColumn("rnk", dense_rank().over(Window.orderBy($"uq_customer_count".desc))) // 总价
  .where($"rnk" <= 5)
  .select("product", "uq_customer_count", "rnk")
  .orderBy("rnk")
  .repartition(1)
  .write
  .mode(SaveMode.Overwrite)
  .format("orc")
  .saveAsTable("ec_ads_report.product_top5_by_uq_customer_cnt")
```



## 九、数据报表

### 9.1、一次性导出数据（MySQL）

```scala
package warehouse

import core.{MysqlConfigFactory, SparkFactory}
import org.apache.log4j.Logger
import org.apache.spark.sql.SparkSession

object ECAdsExport {
  val logger = Logger.getLogger(ECAdsExport.getClass)
  def main(args: Array[String]): Unit = {
    val spark: SparkSession = SparkFactory()
      .build()
      .baseConfig("ec_ads_report")
      .optimizeDriver()
      .optimizeExecutor()
      .optimizeLimit(maxLocalWaitS = 100)
      .optimizeSerializer()
      .optimizeNetAbout()
      .optimizeDynamicAllocation(true)
      .optimizeShuffle()
      .optimizeSpeculation(true)
      .optimizeRuntime(adaptiveEnabled = true, cboEnabled = true)
      .warehouseDir("hdfs://single:9000/hive312/warehouse")
      .end()

    import core.MysqlConfigFactory.Getter
    val dbName: String = "ec_ads_report" // 库名
    // mysql配置
    val conf: Getter = MysqlConfigFactory()
      .build()
      .setDriver("com.mysql.cj.jdbc.Driver")
      .setUrl(s"jdbc:mysql://single:3306/$dbName?createDatabaseIfNotExist=true&useUnicode=true&charSet=utf8") // 库名+建库
      .setUser("zhou") // 远程用户
      .setPassword("Zhou@Remote0032") // 远程密码
      .finish()

    // 将ADS中库导入mysql中，同命库同名表
    spark.sql(s"show tables from $dbName")
      .collect() // 转变：DataFrame => Array[Row]，脱离sparkSession环境，变为一个普通数组
      .foreach(row => {
        val tableName: String = row.getString(1) // 表名
        spark.table(s"$dbName.$tableName") // 获取hbase中所有表（循环）
          .repartition(1)
          // 将表写入mysql中
          .write
          .jdbc(conf.getUrl, tableName, conf.getConf)
      })

    spark.stop()
    logger.info("EC ADS EXPORT TO MYSQL FINISHED")
  }
}
```

### 9.2、任务调度

采用DolphinScheduler进行数据调度，其中需要注意以下几点：

> 在虚拟机中，通过yarn来进行数据资源的调度，因此需要修改：
>
> ```scala
> val spark: SparkSession = SparkFactory()
>   .build()
>   .baseConfig("ec_ods","yarn") # 👈
>   .optimizeDriver()
>   .optimizeExecutor()
>   .optimizeLimit(maxLocalWaitS = 100)
>   .optimizeSerializer()
>   .optimizeNetAbout()
>   .optimizeDynamicAllocation(true)
>   .optimizeShuffle()
>   .optimizeSpeculation(true)
>   .optimizeRuntime(adaptiveEnabled = true, cboEnabled = true)
>   .warehouseDir("hdfs://single:9000/hive312/warehouse")
>   .end()
> ```

修改完此处后，即可进行DolphinScheduler的调度。

yarn进行调度管理分配资源等操作。

**具体操作流程**：

1、修改log4j中的文件路径为虚拟机中的路径：log4j.appender.logfile.File=/root/project_ebs/log/ebs.log

```properties
log4j.rootLogger=ERROR, stdout, logfile
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
log4j.appender.logfile=org.apache.log4j.DailyRollingFileAppender
log4j.appender.logfile.DatePattern='.'yyyy-MM-dd
log4j.appender.logfile.Append=true
log4j.appender.logfile.File=/root/project_ebs/log/ebs.log
log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n
```

2、给每个EBS类添加日志

```scala
import org.apache.log4j.Logger
val logger = Logger.getLogger(EBSOds.getClass)


logger.info("EBS ODS FINISHED")
```

3、修改spark配置（所有类都要改）

原来：（本地）

```java
val spark: SparkSession = SparkFactory()
      .build()
      .baseConfig("ebs_ods")
      .optimizeDriver()
      .optimizeExecutor()
      .optimizeLimit(maxLocalWaitS = 100)
      .optimizeSerializer()
      .optimizeNetAbout()
      .optimizeDynamicAllocation()
      .optimizeShuffle()
      .optimizeSpeculation()
      .warehouseDir("hdfs://single:9000/hive312/warehouse")
      .end()
```

修改后：

```java
val spark: SparkSession = SparkFactory()
      .build()
      .baseConfig("ebs_ods","yarn")
      .optimizeDriver()
      .optimizeExecutor()
      .optimizeLimit(maxLocalWaitS = 100)
      .optimizeSerializer()
      .optimizeNetAbout()
      .optimizeDynamicAllocation(true)
      .optimizeShuffle()
      .optimizeSpeculation(true)
      .optimizeRuntime(adaptiveEnabled = true,cboEnabled = true)
      .warehouseDir("hdfs://single:9000/hive312/warehouse")
      .end()
```

4、将创建好的库表都删除，之后用调度器来统一创建

> 1、删除mysql中的库表：直接删即可
>
> 2、删除hive中的库表：用下面的代码进行删除
>
> 
>
> ```java
> package core
> 
> import org.apache.log4j.Logger
> import org.apache.spark.sql.SparkSession
> 
> object EBSClear {
>   val logger = Logger.getLogger(EBSClear.getClass)
>   def main(args: Array[String]): Unit = {
>     val spark: SparkSession = SparkFactory()
>       .build()
>       .baseConfig("ebs_clear")
>       .optimizeDriver()
>       .optimizeExecutor()
>       .optimizeLimit(maxLocalWaitS = 100)
>       .optimizeSerializer()
>       .optimizeNetAbout()
>       .optimizeDynamicAllocation()
>       .optimizeShuffle()
>       .optimizeSpeculation()
>       .optimizeRuntime(adaptiveEnabled = true,cboEnabled = true)
>       .warehouseDir("hdfs://single:9000/hive312/warehouse")
>       .end()
> 
>     spark.sql("show databases like 'ebs_*'")
>       .collect()
>       .foreach(row=>{
>         val dbName: String = row.getAs("namespace")
>         spark.sql(s"drop database if exists $dbName cascade")
>         logger.info(s"$dbName CLEARED")
>       })
> 
>     spark.stop()
>   }
> }
> ```
>

5、打包：直接点击”package按钮“进行打包即可（将依赖都打进去）

6、在调度器中进行资源上传：warehouse_ebs-1.0-jar-with-dependencies.jar

![](https://gitee.com/LemonWorld/picture/raw/master/img/ebs%E8%B0%83%E5%BA%A6%E5%99%A81.png)

7、进行节点配置：选择spark节点

![](https://gitee.com/LemonWorld/picture/raw/master/img/ebs%E8%B0%83%E5%BA%A6%E5%99%A82.png)

8、保存工作流

![](https://gitee.com/LemonWorld/picture/raw/master/img/ebs%E8%B0%83%E5%BA%A6%E5%99%A83.png)

9、运行

![](https://gitee.com/LemonWorld/picture/raw/master/img/ebs%E8%B0%83%E5%BA%A6%E5%99%A84.png)

追加spark-shuffle配置：

```shell
# 添加spark-on-yarn.jar包
cd /opt/software/spark-3.1.2/yarn
cp spark-3.1.2-yarn-shuffle.jar /opt/software/hadoop-3.1.3/share/hadoop/yarn/lib

# 添加配置：
cd /opt/software/hadoop-3.1.3/etc/hadoop
vim yarn-site.xml
-------------------------------------------------
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle,spark_shuffle</value>
</property>
<property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
    <value>org.apache.spark.network.yarn.YarnShuffleService</value>
</property>
--------------------------------------------------
```

启动服务：

![](https://gitee.com/LemonWorld/picture/raw/master/img/ebs%E8%B0%83%E5%BA%A6%E5%99%A8%E6%93%8D%E4%BD%9C.png)



### 9.3、数据呈现

在大数据的离线数仓中，我们使用FineReport来进行数据的呈现（UI呈现）。

![](https://gitee.com/LemonWorld/picture/raw/master/img/FineShow1.png)

![](https://gitee.com/LemonWorld/picture/raw/master/img/FineShow2.png)

![](https://gitee.com/LemonWorld/picture/raw/master/img/FineShow3.png)